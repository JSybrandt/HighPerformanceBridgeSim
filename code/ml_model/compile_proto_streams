#!/usr/bin/env python3
from result_pb2 import Trial
import stream  # proto stream
from glob import glob
from multiprocessing import Pool
from random import shuffle
import numpy as np
from pathlib import Path
from argparse import ArgumentParser
import pickle
from tqdm import tqdm
from functools import partial
from multiprocessing import current_process

DISABLE_PBAR=False

def parse_args():
  parser = ArgumentParser()
  parser.add_argument("in_dir", type=Path)
  parser.add_argument("out_path", type=Path)
  parser.add_argument("--validation_ratio", type=float, default=0.1)
  parser.add_argument("--test_ratio", type=float, default=0.2)
  parser.add_argument("--data_per_day", type=int)
  parser.add_argument("--target_type", choices=["classification", "regression"], default="classification")
  return parser.parse_args()

def to_sequence(trial):
  seq_length = len(trial.fft.upper_amp)
  assert seq_length == len(trial.fft.lower_amp)
  assert seq_length > 0
  num_features = 6
  if trial.HasField("temperature"):
    num_features += 1
  sequence = np.empty((seq_length, num_features), dtype=np.float32)
  band_pass_step = (trial.fft.band_pass_end - trial.fft.band_pass_start) \
                 / seq_length
  for idx, freq in enumerate(np.arange(trial.fft.band_pass_start,
                                       trial.fft.band_pass_end,
                                       band_pass_step)):
    sequence[idx, :] = [trial.time_of_day,
                        trial.vehicle.mass,
                        trial.vehicle.speed,
                        freq,
                        trial.fft.upper_amp[idx],
                        trial.fft.lower_amp[idx]] \
                     + ([trial.temperature] \
                       if trial.HasField("temperature") \
                       else [])
  return sequence

def load_stream(stream_path, target_type, data_per_day):
  res = []
  # try:
  with stream.open(str(stream_path), 'rb') as proto_stream:
    for idx, data in enumerate(proto_stream):
      trial = Trial()
      trial.ParseFromString(data)
      assert trial.HasField("damage_class")
      assert trial.HasField("damage_amount")
      # COMPRESS DAMAGE CLASSES
      if trial.damage_class > 0:
        if trial.damage_class <= 3:
          trial.damage_class = 1
        else:
          trial.damage_class = 2
      res.append({'x': to_sequence(trial),
                  'y': trial.damage_class,
                  'damage_class': trial.damage_class,
                  'damage_amount': trial.damage_amount,
                  'day': trial.day})
      del trial
      if data_per_day is not None and num_complete >= data_per_day:
        break
  # except Exception as e:
    # print("Error with file:", stream_path, e)
  return res

def get_empty_output_dict():
  return {
      "train": [],
      "test": [],
      "validation": []
    }

def random_split(unsorted_data,validation_ratio, test_ratio):
  output = get_empty_output_dict()
  shuffle(unsorted_data)
  train_cutoff = int(len(unsorted_data) * (1 - validation_ratio - test_ratio))
  validation_cutoff = int(train_cutoff + len(unsorted_data)*validation_ratio)
  assert train_cutoff > 0
  assert validation_cutoff >= train_cutoff
  output["test"] = unsorted_data[validation_cutoff:]
  del unsorted_data[validation_cutoff:]
  output["validation"] = unsorted_data[train_cutoff:]
  del unsorted_data[train_cutoff:]
  output["train"] = unsorted_data
  return output

if __name__=="__main__":
  args = parse_args()
  print(args)
  stream_paths = list(args.in_dir.glob("*.proto_stream"))
  assert len(stream_paths) > 0
  print("Found", len(stream_paths))
  assert not args.out_path.exists() and args.out_path.parent.is_dir()

  load_fn = partial(load_stream,
                    target_type=args.target_type,
                    data_per_day=args.data_per_day)

  split_fn = partial(random_split,
                     validation_ratio=args.validation_ratio,
                     test_ratio=args.test_ratio)

  print("Loading Each Day")
  with Pool() as pool:
    data_per_day = pool.map(load_fn, stream_paths)
  print("Stacking days together")
  data = []
  for day_data in tqdm(data_per_day):
    data.extend(day_data)
    del day_data

  print("Shuffling")
  output = split_fn(data)
  output["description"] = str(args)
  print("Saving")
  with open(args.out_path, 'wb') as pfile:
    pickle.dump(output, pfile)
