#!/usr/bin/env python3
import tensorflow as tf
import pickle
import numpy as np
from argparse import ArgumentParser
from pathlib import Path
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.externals import joblib
from sklearn.utils.class_weight import compute_class_weight
from imblearn.keras import balanced_batch_generator
from imblearn.under_sampling import RandomUnderSampler

def parse_args():
  parser = ArgumentParser()
  parser.add_argument("input_pickle", type=Path)
  parser.add_argument("--model_out", type=Path)
  parser.add_argument("--normalizer_out", type=Path)
  parser.add_argument("--batch_size", type=int, default=128)
  parser.add_argument("--epochs", type=int, default=10000)
  parser.add_argument("--sequence_limit", type=int, default=140)
  parser.add_argument("--training_detail_csv", type=Path)
  return parser.parse_args()

def get_model(seq_length, num_features, num_classes):
  input_layer = tf.keras.layers.Input((seq_length, num_features))
  hl = tf.keras.layers.Conv1D(16, 3,
                              activation="relu")(input_layer)
  hl = tf.keras.layers.Conv1D(16, 3,
                              activation="relu")(hl)
  hl = tf.keras.layers.MaxPooling1D()(hl)
  hl = tf.keras.layers.Conv1D(32, 3,
                              activation="relu")(hl)
  hl = tf.keras.layers.Conv1D(32, 3,
                              activation="relu")(hl)
  hl = tf.keras.layers.MaxPooling1D()(hl)
  hl = tf.keras.layers.Conv1D(64, 3,
                              activation="relu")(hl)
  hl = tf.keras.layers.Conv1D(64, 3,
                              activation="relu")(hl)
  hl = tf.keras.layers.MaxPooling1D()(hl)
  hl = tf.keras.layers.Flatten()(hl)
  hl = tf.keras.layers.Dense(512, activation="relu")(hl)
  hl = tf.keras.layers.Dense(512, activation="relu")(hl)

  # hl = tf.keras.layers.LSTM(256,
                                      # #return_sequences=True,
                                      # unroll=True,
                                      # recurrent_dropout=0.5)(hl)
  # hl = tf.keras.layers.Concatenate()(
                   # tf.keras.layers.LSTM(256,
                                        # unroll=True,
                                        # recurrent_dropout=0.5,
                                        # dropout=0.5,
                                        # return_state=True)(hl))
  class_out = tf.keras.layers.Dense(num_classes,
                                    activation="softmax",
                                    name="damage_class")(hl)
  model = tf.keras.models.Model(input_layer, class_out)
  return model

if __name__=="__main__":
  args = parse_args()
  print("Testing paths")
  assert args.input_pickle.is_file()
  print("Testing params")
  assert args.batch_size > 0
  assert args.epochs > 0

  print("Loading data")
  with open(args.input_pickle, 'rb') as pfile:
    raw_data = pickle.loads(pfile.read())

  print("Loading data")
  splits = ["train", "validation"]
  data = {}
  for split in splits:
    assert split in raw_data
    data[split] = {
        "x": [],
        "damage_class":[],
        "damage_amount":[]
      }
    for pt in raw_data[split]:
      data[split]["x"].append(pt["x"])
      data[split]["damage_class"].append(pt["damage_class"])
      data[split]["damage_amount"].append(pt["damage_amount"])
    # Convert to np
    data[split]["x"] = tf.keras.preprocessing.sequence.pad_sequences(
        data[split]["x"], args.sequence_limit, dtype=float)
    # To one-hot
    data[split]["damage_class"] = tf.keras.utils.to_categorical(
        data[split]["damage_class"],
        num_classes = 6)
    # to np
    data[split]["damage_amount"] = np.array(
        data[split]["damage_amount"],
        dtype=float).reshape(-1, 1)

  print("Normalizing")
  num_features = data["train"]["x"].shape[2]
  feature_normalizer = StandardScaler(copy=False)
  damage_amount_normalizer = MinMaxScaler(copy=False)
  feature_normalizer.fit(data["train"]["x"].reshape(-1, num_features))
  damage_amount_normalizer.fit(data["train"]["damage_amount"])
  for split in splits:
    # Normalizing features
    original_shape = data[split]["x"].shape
    data[split]["x"] = feature_normalizer.transform(
        data[split]['x'].reshape(-1, num_features)
    ).reshape(original_shape)
    # Normalizing damage_amount
    data[split]["damage_amount"] = damage_amount_normalizer.transform(
        data[split]["damage_amount"])
  print("Saving normalizers")
  joblib.dump({"damage_amount_normalizer": damage_amount_normalizer,
               "feature_normalizer": feature_normalizer},
               args.normalizer_out)

  # print("Getting class weights")
  class_vals = data["train"]["damage_class"].argmax(axis=1)
  classes = np.unique(class_vals)
  # print(classes)
  # weights = compute_class_weight("balanced",
                                       # classes,
                                       # class_vals)
  # print(weights)
  # class_weight = {c:w for c, w in zip(classes,weights)}
  # print(class_weight)

  if args.model_out.is_file():
    print("Loading existing model.")
    model = tf.keras.models.load_model(str(args.model_out))
  else:
    print("Creating model.")
    model = get_model(args.sequence_limit, num_features, 6)
    model.summary()

  model = tf.keras.utils.multi_gpu_model(model, 2)
  model.compile(
      optimizer=tf.keras.optimizers.Adam(),
      loss = "categorical_crossentropy",
      metrics = ["accuracy"]
    )

  callbacks = [tf.keras.callbacks.EarlyStopping(monitor="val_loss",
                                                patience=10),
               tf.keras.callbacks.TerminateOnNaN(),
               tf.keras.callbacks.ModelCheckpoint(str(args.model_out),
                                                  monitor="val_loss",
                                                  save_best_only=True)]
  if args.training_detail_csv is not None:
    callbacks.append(tf.keras.callbacks.CSVLogger(str(args.training_detail_csv)))

  model.fit(
      data["train"]["x"],
      data["train"]["damage_class"],
      batch_size=args.batch_size,
      epochs=args.epochs,
      validation_data=(data["validation"]["x"],
                       data["validation"]["damage_class"]),
      shuffle=True,
      callbacks=callbacks
      )
  # We use callback to save best

